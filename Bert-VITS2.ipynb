{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RomxmJkjnqE",
        "outputId": "6eaeb474-9664-4e7a-ef8a-40895e12a961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 31 04:37:17 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS4NwdX8fQhg",
        "outputId": "cb58ba27-8d1b-4a77-db1f-2d0877a3cde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title STEP 0 連接 Google Drive\n",
        "#@markdown #STEP 0\n",
        "#@markdown ##連接 Google Drive\n",
        "#@markdown ##Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu-XmSvFj1gg"
      },
      "source": [
        "#STEP 0 確認有使用GPU\n",
        "##記事本必須在GPU模式下運行，可以執行上方的nvidia-smi查看GPU是否有GPU。若没有，點擊左上角的 編輯>筆記本設定，把硬體加速器改成GPU。\n",
        "##如記事本有出問題可在以下儲存庫回報\n",
        "> https://github.com/ADT109119/Bert-VITS2-Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f2PYE5UE4cCc",
        "outputId": "45f907e1-58cb-44dd-fa11-9e1b88aa09ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bert-VITS2'...\n",
            "remote: Enumerating objects: 2455, done.\u001b[K\n",
            "remote: Counting objects: 100% (283/283), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 2455 (delta 147), reused 219 (delta 115), pack-reused 2172\u001b[K\n",
            "Receiving objects: 100% (2455/2455), 9.66 MiB | 11.34 MiB/s, done.\n",
            "Resolving deltas: 100% (1435/1435), done.\n",
            "/content/Bert-VITS2\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 1))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.58.1)\n",
            "Collecting phonemizer (from -r requirements.txt (line 5))\n",
            "  Downloading phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.11.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.15.1)\n",
            "Collecting Unidecode (from -r requirements.txt (line 8))\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting amfm_decompy (from -r requirements.txt (line 9))\n",
            "  Downloading AMFM_decompy-1.0.11.tar.gz (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.5/751.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.42.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.35.2)\n",
            "Collecting pypinyin (from -r requirements.txt (line 12))\n",
            "  Downloading pypinyin-0.50.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cn2an (from -r requirements.txt (line 13))\n",
            "  Downloading cn2an-0.5.22-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.50.2 (from -r requirements.txt (line 14))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av (from -r requirements.txt (line 15))\n",
            "  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mecab-python3 (from -r requirements.txt (line 16))\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru (from -r requirements.txt (line 17))\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidic-lite (from -r requirements.txt (line 18))\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cmudict (from -r requirements.txt (line 19))\n",
            "  Downloading cmudict-1.0.16-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugashi (from -r requirements.txt (line 20))\n",
            "  Downloading fugashi-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words (from -r requirements.txt (line 21))\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (2.31.0)\n",
            "Collecting pyopenjtalk-prebuilt (from -r requirements.txt (line 24))\n",
            "  Downloading pyopenjtalk_prebuilt-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaconv (from -r requirements.txt (line 25))\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (5.9.5)\n",
            "Collecting GPUtil (from -r requirements.txt (line 27))\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vector_quantize_pytorch (from -r requirements.txt (line 28))\n",
            "  Downloading vector_quantize_pytorch-1.12.5-py3-none-any.whl (24 kB)\n",
            "Collecting g2p_en (from -r requirements.txt (line 29))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from -r requirements.txt (line 30))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pykakasi (from -r requirements.txt (line 31))\n",
            "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langid (from -r requirements.txt (line 32))\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 1))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 1)) (23.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (1.10.13)\n",
            "Collecting pydub (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->-r requirements.txt (line 14)) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio==3.50.2->-r requirements.txt (line 14)) (2023.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 4)) (0.41.1)\n",
            "Collecting segments (from phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer->-r requirements.txt (line 5)) (23.1.0)\n",
            "Collecting dlinfo (from phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 11)) (4.66.1)\n",
            "Collecting proces>=0.1.3 (from cn2an->-r requirements.txt (line 13))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->-r requirements.txt (line 19)) (7.0.0)\n",
            "Collecting docopt>=0.6.2 (from num2words->-r requirements.txt (line 21))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 23)) (2023.11.17)\n",
            "Requirement already satisfied: cython>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pyopenjtalk-prebuilt->-r requirements.txt (line 24)) (3.0.6)\n",
            "Collecting einops>=0.7.0 (from vector_quantize_pytorch->-r requirements.txt (line 28))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vector_quantize_pytorch->-r requirements.txt (line 28)) (2.1.0+cu121)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 29)) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 29)) (7.0.0)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 29))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi->-r requirements.txt (line 31))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->-r requirements.txt (line 19)) (3.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 29)) (8.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.50.2->-r requirements.txt (line 14)) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 1)) (4.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 1)) (1.16.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykakasi->-r requirements.txt (line 31)) (1.14.1)\n",
            "Collecting starlette<0.33.0,>=0.29.0 (from fastapi->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->-r requirements.txt (line 14)) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.50.2->-r requirements.txt (line 14))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->-r requirements.txt (line 14)) (1.3.0)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading clldutils-3.22.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting csvw>=1.5.6 (from segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading csvw-3.2.1-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 1)) (2.21)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5)) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading bibtexparser-2.0.0b4-py3-none-any.whl (37 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer->-r requirements.txt (line 5)) (4.9.3)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5)) (2.14.0)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting language-tags (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5))\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer->-r requirements.txt (line 5)) (4.1.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->-r requirements.txt (line 14)) (0.15.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.50.2->-r requirements.txt (line 14)) (1.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->vector_quantize_pytorch->-r requirements.txt (line 28)) (1.3.0)\n",
            "Building wheels for collected packages: amfm_decompy, unidic-lite, jaconv, GPUtil, langid, distance, docopt, ffmpy, pylatexenc\n",
            "  Building wheel for amfm_decompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amfm_decompy: filename=AMFM_decompy-1.0.11-py3-none-any.whl size=42835 sha256=d2e3019e855b5403512d788699b147d1eaf469712e8463d55c0618959f82064e\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/81/e7/443ad333f2f4ed8c06fc027caeb0d0c84b896fe7e56c2e92b1\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=80e6dd6aac02318a3c08cad4dd453295675c1990fcb5fc0a64cb15968df10782\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16415 sha256=e3d17232f957d74286af23964c4cb1c641052608ebb1f11a415840717f2ef25d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=2c5501c798cfe85efc50656767581c90a01fc92d844857f4115f524d641afffc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=c31b20a91fe2b1c0f2eff89ab5c136f57d91ffa5cb86cce645aff716608ee198\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=0d08fef676977ec23af5b831f716176d201f29f7e863de819be69a0d0e9303d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0287c3c0ba8194aad394a3bef19fd2aeb7c727e4e028790d968d4a11435835ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=e4124671ad228b0c9c0a887712ef7dbf7935845a45070b1007885555a6e82828\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=ba304bae75b1584aef678f89507c601d9cb5fc77bfa51a6fc2231f346299a83e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built amfm_decompy unidic-lite jaconv GPUtil langid distance docopt ffmpy pylatexenc\n",
            "Installing collected packages: unidic-lite, sentencepiece, rfc3986, pylatexenc, pydub, mecab-python3, language-tags, jaconv, GPUtil, ffmpy, docopt, dlinfo, distance, websockets, Unidecode, typing-extensions, semantic-version, python-multipart, pypinyin, pyopenjtalk-prebuilt, proces, orjson, num2words, loguru, langid, isodate, h11, fugashi, einops, deprecated, colorlog, colorama, bibtexparser, av, aiofiles, uvicorn, starlette, resampy, rdflib, pykakasi, httpcore, cn2an, cmudict, clldutils, amfm_decompy, vector_quantize_pytorch, librosa, httpx, fastapi, gradio-client, g2p_en, csvw, segments, gradio, phonemizer\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.1\n",
            "    Uninstalling librosa-0.10.1:\n",
            "      Successfully uninstalled librosa-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GPUtil-1.4.0 Unidecode-1.3.7 aiofiles-23.2.1 amfm_decompy-1.0.11 av-11.0.0 bibtexparser-2.0.0b4 clldutils-3.22.1 cmudict-1.0.16 cn2an-0.5.22 colorama-0.4.6 colorlog-6.8.0 csvw-3.2.1 deprecated-1.2.14 distance-0.1.3 dlinfo-1.2.1 docopt-0.6.2 einops-0.7.0 fastapi-0.108.0 ffmpy-0.3.1 fugashi-1.3.0 g2p_en-2.1.0 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 isodate-0.6.1 jaconv-0.3.4 langid-1.1.6 language-tags-1.2.0 librosa-0.9.2 loguru-0.7.2 mecab-python3-1.0.8 num2words-0.5.13 orjson-3.9.10 phonemizer-3.2.1 proces-0.1.7 pydub-0.25.1 pykakasi-2.2.1 pylatexenc-2.10 pyopenjtalk-prebuilt-0.3.0 pypinyin-0.50.0 python-multipart-0.0.6 rdflib-7.0.0 resampy-0.4.2 rfc3986-1.5.0 segments-2.2.1 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.32.0.post1 typing-extensions-4.9.0 unidic-lite-1.0.8 uvicorn-0.25.0 vector_quantize_pytorch-1.12.5 websockets-11.0.3\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
            "--2023-12-31 04:38:36--  https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.23, 18.164.174.118, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/microsoft/wavlm-base-plus/3bb273a6ace99408b50cfc81afdbb7ef2de02da2eab0234e18db608ce692fe51?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1704256716&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNDI1NjcxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvd2F2bG0tYmFzZS1wbHVzLzNiYjI3M2E2YWNlOTk0MDhiNTBjZmM4MWFmZGJiN2VmMmRlMDJkYTJlYWIwMjM0ZTE4ZGI2MDhjZTY5MmZlNTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S3N38rzrz9wjTTRDskU7qkZCSZJQ0LLRw45xWmNvBjqQAzQsSWKEpIYBEEEMl5VhHiC54PmeJ6dz0YqmLyVPYyPLLKE13Zck9zXUDpdPS%7EtOXDWfm2E-bbbUELkKhG3vh8BOsT2kscdXN7P08Gg9wl3HquKrMV6i88S6b%7EwM5gEKPxC3L6D%7ExxHppRggXIrti4fSnwXvfBvFEl2QfyJnSgkGSSKgqdu089QKt2rNrrecYpEg2BnFp%7EdyMVCqTryNiHjm4w3paumCycAkefG2eW-jQL5z3MRL7hbLlJuMjEGgkM%7ERUUmzEWjBQyScp-ib0F8OJTBIUY7bajGfYxPdLA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-12-31 04:38:36--  https://cdn-lfs.huggingface.co/microsoft/wavlm-base-plus/3bb273a6ace99408b50cfc81afdbb7ef2de02da2eab0234e18db608ce692fe51?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1704256716&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNDI1NjcxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9taWNyb3NvZnQvd2F2bG0tYmFzZS1wbHVzLzNiYjI3M2E2YWNlOTk0MDhiNTBjZmM4MWFmZGJiN2VmMmRlMDJkYTJlYWIwMjM0ZTE4ZGI2MDhjZTY5MmZlNTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=S3N38rzrz9wjTTRDskU7qkZCSZJQ0LLRw45xWmNvBjqQAzQsSWKEpIYBEEEMl5VhHiC54PmeJ6dz0YqmLyVPYyPLLKE13Zck9zXUDpdPS%7EtOXDWfm2E-bbbUELkKhG3vh8BOsT2kscdXN7P08Gg9wl3HquKrMV6i88S6b%7EwM5gEKPxC3L6D%7ExxHppRggXIrti4fSnwXvfBvFEl2QfyJnSgkGSSKgqdu089QKt2rNrrecYpEg2BnFp%7EdyMVCqTryNiHjm4w3paumCycAkefG2eW-jQL5z3MRL7hbLlJuMjEGgkM%7ERUUmzEWjBQyScp-ib0F8OJTBIUY7bajGfYxPdLA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.206.4, 18.154.206.17, 18.154.206.28, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.206.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 377617425 (360M) [application/octet-stream]\n",
            "Saving to: ‘slm/wavlm-base-plus/pytorch_model.bin’\n",
            "\n",
            "slm/wavlm-base-plus 100%[===================>] 360.12M   198MB/s    in 1.8s    \n",
            "\n",
            "2023-12-31 04:38:38 (198 MB/s) - ‘slm/wavlm-base-plus/pytorch_model.bin’ saved [377617425/377617425]\n",
            "\n",
            "Cloning into 'bert/chinese-roberta-wwm-ext-large'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Total 50 (delta 0), reused 0 (delta 0), pack-reused 50\u001b[K\n",
            "Unpacking objects: 100% (50/50), 156.68 KiB | 1.84 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 3.64 GiB | 47.49 MiB/s, done.\n",
            "Cloning into 'bert/deberta-v2-large-japanese-char-wwm'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 11\u001b[K\n",
            "Unpacking objects: 100% (14/14), 59.91 KiB | 7.49 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 2.45 GiB | 62.82 MiB/s, done.\n",
            "Cloning into 'bert/deberta-v3-large'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Total 75 (delta 0), reused 0 (delta 0), pack-reused 75\u001b[K\n",
            "Unpacking objects: 100% (75/75), 9.99 KiB | 465.00 KiB/s, done.\n",
            "Filtering content: 100% (4/4), 2.96 GiB | 59.63 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "#@title STEP 1 複製儲存庫並安裝必要的函式庫\n",
        "#@markdown #STEP 1\n",
        "#@markdown ##複製儲存庫並安裝必要的函式庫\n",
        "#@markdown ##Clone repository & Install requirements lib\n",
        "\n",
        "!git clone https://github.com/fishaudio/Bert-VITS2.git\n",
        "%cd ./Bert-VITS2/\n",
        "!pip install -r requirements.txt\n",
        "!pip install pyyaml\n",
        "\n",
        "#下載 Bert 以及 wavlm 模型\n",
        "!wget https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin?download=true -O slm/wavlm-base-plus/pytorch_model.bin\n",
        "\n",
        "!rm -rf bert/chinese-roberta-wwm-ext-large\n",
        "!git clone https://huggingface.co/hfl/chinese-roberta-wwm-ext-large bert/chinese-roberta-wwm-ext-large\n",
        "\n",
        "!rm -rf bert/deberta-v2-large-japanese-char-wwm\n",
        "!git clone https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm bert/deberta-v2-large-japanese-char-wwm\n",
        "\n",
        "!rm -rf bert/deberta-v3-large\n",
        "!git clone https://huggingface.co/microsoft/deberta-v3-large bert/deberta-v3-large\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ5CZVxxRlpx"
      },
      "source": [
        "# Bert-VITS2 数据预处理\n",
        "\n",
        "数据准备：\n",
        "将数据放置在 data 文件夹下，按照如下结构组织：\n",
        "\n",
        "```\n",
        "├── data\n",
        "│   ├── {你的数据集名称}\n",
        "│   │   ├── esd.list\n",
        "│   │   ├── raw\n",
        "│   │   │   ├── ****.wav\n",
        "│   │   │   ├── ****.wav\n",
        "│   │   │   ├── ...\n",
        "```\n",
        "\n",
        "其中，`raw` 文件夹下保存所有的音频文件，`esd.list` 文件为标签文本，格式为\n",
        "\n",
        "```\n",
        "****.wav|{说话人名}|{语言 ID}|{标签文本}\\n\n",
        "```\n",
        "\n",
        "例如：\n",
        "```\n",
        "vo_ABDLQ001_1_paimon_02.wav|派蒙|ZH|没什么没什么，只是平时他总是站在这里，有点奇怪而已\n",
        "noa_501_0001.wav|NOA|JP|そうだね、油断しないのはとても大事なことだと思う\\n\"\n",
        "Albedo_vo_ABDLQ002_4_albedo_01.wav|Albedo|EN|Who are you? Why did you alarm them?\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edd8YOVJr7hh",
        "outputId": "485c2812-2984-4377-e108-10cdf7f5d9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP 2-1 生成上傳按鈕載入資料集\n",
        "#@markdown #STEP 2-1\n",
        "#@markdown ##生成上傳按鈕載入資料集\n",
        "#@markdown ##Upload Dataset\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "uploaded = files.upload()\n",
        "basepath = os.getcwd()\n",
        "upload_path = \"./\"\n",
        "for filename in uploaded.keys():\n",
        "  shutil.move(os.path.join(basepath, filename), os.path.join(upload_path, \"data.zip\"))\n"
      ],
      "metadata": {
        "id": "43NK2XSH8cgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tXYzufRyIsy"
      },
      "outputs": [],
      "source": [
        "#@title STEP 2-2 從Google Drive載入資料集\n",
        "#@markdown #STEP 2-2\n",
        "#@markdown ##載入資料集\n",
        "#@markdown ##Upload Dataset from Google Drive\n",
        "\n",
        "#@markdown ###資料集路徑\n",
        "data_set_zip_file = \"/content/drive/MyDrive/Bert-VITS2/data.zip\" #@param {type:\"string\"}\n",
        "\n",
        "!cp -rf {data_set_zip_file} ./data.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP 2-3 上傳音檔 透過Whisper生成字幕再自動切分 (因準確度關係 不建議使用)\n",
        "#@markdown #STEP 2-2\n",
        "#@markdown ##透過Whisper生成字幕再自動切分\n",
        "#@markdown ##(因準確度關係 不建議使用)\n",
        "#@markdown ##Upload audios and generate subtitle by OpenAI Whisper to split audios\n",
        "#@markdown ##(Not recomand)\n",
        "#@markdown ##音檔請依照 `{說話者名稱}_{語言}_{隨便的編號}` 格式\n",
        "\n",
        "!mkdir upload_audios\n",
        "!mkdir upload_audios/audio\n",
        "!mkdir upload_audios/srt\n",
        "\n",
        "upload_from_google_drive = True # @param {type:\"boolean\"}\n",
        "\n",
        "if upload_from_google_drive:\n",
        "  data_folder_path = \"/content/drive/MyDrive/Bert-VITS2/audio\" #@param {type:\"string\"}\n",
        "  !cp -rf {data_folder_path} ./upload_audios/audio\n",
        "else:\n",
        "  from google.colab import files\n",
        "  import shutil\n",
        "  import os\n",
        "  uploaded = files.upload()\n",
        "  basepath = os.getcwd()\n",
        "  upload_path = \"./upload_audios/audio\"\n",
        "  for filename in uploaded.keys():\n",
        "    shutil.move(os.path.join(basepath, filename), os.path.join(upload_path, filename))\n",
        "\n",
        "!pip install openai-whisper\n",
        "import glob\n",
        "\n",
        "file_list = glob.glob(\"./upload_audios/audio/*.wav\")\n",
        "for path in file_list:\n",
        "  !whisper {path} --model large-v3 --output_dir \"./upload_audios/srt\"\n",
        "\n",
        "!git clone https://github.com/ADT109119/Bert-VITS2-colab.git ./upload_audios\n",
        "!python ./upload_audios/processing.py\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_folder_with_file(folder_path, file_path, output_path):\n",
        "    with shutil.ZipFile(output_path, 'w') as zipf:\n",
        "        zipf.write(folder_path, arcname=os.path.basename(folder_path))\n",
        "        zipf.write(file_path, arcname=os.path.basename(file_path))\n",
        "\n",
        "zip_folder_with_file('./upload_audios/raw', './upload_audios/esd.list', './data.zip')\n"
      ],
      "metadata": {
        "id": "2N976Ct850V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP 2.1 解壓縮資料集\n",
        "#@markdown #STEP 231\n",
        "#@markdown ##解壓縮資料集\n",
        "#@markdown ##Unzip Data\n",
        "\n",
        "#@markdown ###資料集名稱\n",
        "!mkdir data\n",
        "\n",
        "data_dir = \"dataset\" #@param {type:\"string\"}\n",
        "!unzip ./data.zip -d ./data/{data_dir}"
      ],
      "metadata": {
        "id": "gF34yVSD-b0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiFJX0-d6thC"
      },
      "outputs": [],
      "source": [
        "#@title STEP 2.5 處理資料\n",
        "#@markdown #STEP 2.5\n",
        "#@markdown ##處理資料\n",
        "#@markdown ##Process Data\n",
        "from webui_preprocess import generate_config, resample, preprocess_text, bert_gen\n",
        "\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "\n",
        "print(generate_config(data_dir, batch_size))\n",
        "print(resample(data_dir))\n",
        "print(preprocess_text(data_dir))\n",
        "print(bert_gen(data_dir))\n",
        "\n",
        "#修改 config.yml\n",
        "import yaml\n",
        "\n",
        "def load_yaml(file_path):\n",
        "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
        "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
        "    return data\n",
        "\n",
        "def save_yaml(data, file_path):\n",
        "    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
        "        yaml.dump(data, file, default_flow_style=False)\n",
        "\n",
        "# 載入 YAML 檔案\n",
        "yaml_file_path = 'config.yml'\n",
        "yaml_data = load_yaml(yaml_file_path)\n",
        "\n",
        "# 修改 YAML 中的內容\n",
        "yaml_data['dataset_path'] = f\"data/{data_dir}\"\n",
        "yaml_data['train_ms']['config_path'] = \"configs/config.json\"\n",
        "\n",
        "# 儲存修改後的 YAML 檔案\n",
        "save_yaml(yaml_data, yaml_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgvT1jqef9Xy"
      },
      "outputs": [],
      "source": [
        "#@title STEP 2.6 自動儲存步數修改\n",
        "#@markdown #STEP 2.6\n",
        "#@markdown ##自動儲存步數修改\n",
        "\n",
        "#@markdown ###每隔n步自動儲存\n",
        "save_step = 200 # @param {type:\"integer\"}\n",
        "\n",
        "import json\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def save_json(data, file_path):\n",
        "    with open(file_path, 'w', encoding=\"utf-8\") as file:\n",
        "        json.dump(data, file, indent=2)\n",
        "\n",
        "# 載入 JSON 檔案\n",
        "json_file_path = f\"data/{data_dir}/configs/config.json\"\n",
        "json_data = load_json(json_file_path)\n",
        "\n",
        "# 修改 JSON 中的內容\n",
        "json_data['train']['log_interval'] = save_step\n",
        "json_data['train']['eval_interval'] = save_step\n",
        "\n",
        "# 儲存修改後的 JSON 檔案\n",
        "save_json(json_data, json_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_vf9UZuRUPD"
      },
      "source": [
        "# STEP 3 載入模型\n",
        "## 3-1、3-2 則一"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfCOo-7YTCOK"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3-1 下載預訓練模型\n",
        "!git clone https://huggingface.co/OedoSoldier/Bert-VITS2-2.3 data/{data_dir}/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sKnejk_e2V6"
      },
      "outputs": [],
      "source": [
        "#@title STEP 3-2 載入上次訓練到一半 儲存在 Google Drive 的模型\n",
        "!cp -f ../drive/MyDrive/Bert-VITS2/models/* data/{data_dir}/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HF1Uwy0UUZA"
      },
      "source": [
        "#STEP 4 開始訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EMbEm9Fy0DhA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TENSORBOARD_BINARY'] = '/usr/local/bin/tensorboard'\n",
        "%reload_ext tensorboard\n",
        "model_dir = f\"./data/{data_dir}/models\"\n",
        "%tensorboard --logdir $model_dir\n",
        "\n",
        "!torchrun --nproc_per_node=1 train_ms.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP5QJC7Q0eS"
      },
      "source": [
        "#STEP 5 推理測試\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBIfMtsH74u2"
      },
      "outputs": [],
      "source": [
        "#@title STEP 5 推理測試\n",
        "#@markdown #STEP 5\n",
        "#@markdown ##推理測試\n",
        "\n",
        "#@markdown ###\n",
        "#@markdown ###推理所使用的模型步數\n",
        "model_step = 2000 # @param {type:\"integer\"}\n",
        "import yaml\n",
        "def load_yaml(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
        "    return data\n",
        "def save_yaml(data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        yaml.dump(data, file, default_flow_style=False)\n",
        "# 載入 YAML 檔案\n",
        "yaml_file_path = 'config.yml'\n",
        "yaml_data = load_yaml(yaml_file_path)\n",
        "# 修改 YAML 中的內容\n",
        "yaml_data['dataset_path'] = f\"data/{data_dir}\"\n",
        "yaml_data['webui']['share'] = \"true\"\n",
        "yaml_data['webui']['model'] = f\"models/G_{model_step}.pth\"\n",
        "yaml_data['webui']['config_path'] = \"configs/config.json\"\n",
        "# 儲存修改後的 YAML 檔案\n",
        "save_yaml(yaml_data, yaml_file_path)\n",
        "!torchrun --nproc_per_node=1 webui.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7VH8k-s0G71"
      },
      "source": [
        "#STEP 6 保存到Google Drive(自選)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2StL2Wqt0MAt"
      },
      "outputs": [],
      "source": [
        "#@title STEP 6.1 連接Google Drive\n",
        "#@markdown ##如果前面連接過了 這邊就不用再連一次\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72nLh_L50g74"
      },
      "outputs": [],
      "source": [
        "#@title STEP 6.2 複製檔案到Google Drive\n",
        "#@markdown ###模型儲存路徑\n",
        "Google_Drive_Folder = \"/content/drive/MyDrive/Bert-VITS2\" #@param {type:\"string\"}\n",
        "#@markdown ###儲存模型步數\n",
        "steps = 2000 # @param {type:\"integer\"}\n",
        "!mkdir {Google_Drive_Folder}/models\n",
        "!cp -rf /content/Bert-VITS2/data/{data_dir}/models/G_{steps}.pth {Google_Drive_Folder}/models/\n",
        "!cp -rf /content/Bert-VITS2/data/{data_dir}/models/D_{steps}.pth {Google_Drive_Folder}/models/\n",
        "!cp -rf /content/Bert-VITS2/data/{data_dir}/models/WD_{steps}.pth {Google_Drive_Folder}/models/\n",
        "!cp -rf /content/Bert-VITS2/data/{data_dir}/models/DUR_{steps}.pth {Google_Drive_Folder}/models/\n",
        "!cp -rf /content/Bert-VITS2/data/{data_dir}/configs {Google_Drive_Folder}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STEP 6-2 下載模型"
      ],
      "metadata": {
        "id": "iXOlMMjr_7tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(f\"./data/{data_dir}/models/G_{steps}.pth\")\n",
        "files.download(f\"./data/{data_dir}/models/D_{steps}.pth\")\n",
        "files.download(f\"./data/{data_dir}/models/WD_{steps}.pth\")\n",
        "files.download(f\"./data/{data_dir}/models/DUR_{steps}.pth\")\n",
        "files.download(f\"./data/{data_dir}/configs/config.json\")"
      ],
      "metadata": {
        "id": "q5LOtjRo_pls"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}